"""Comprehensive stress tests for Contract Evaluation Agent.

These tests validate schemas, edge cases, and logic without requiring
the full Hive framework to be running.
"""

import json
from datetime import date
from pathlib import Path


# Test 1: Schema Validation
def test_schemas():
    """Test Pydantic schema validation and edge cases."""
    print("=" * 60)
    print("TEST 1: Schema Validation")
    print("="*60)
    
    try:
        # Import schemas directly (requires pydantic)
        import sys
        sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent))
        
        from examples.templates.contract_evaluator.schemas import (
            ContractMetadata,
            ContractType,
            Jurisdiction,
            ConfidentialityFinding,
            LiabilityFinding,
            TermObligation,
            RiskAssessment,
        )
        
        print("‚úì All schemas imported successfully")
        
        # Test valid metadata
        metadata = ContractMetadata(
            contract_id="test_001",
            contract_type=ContractType.MUTUAL_NDA,
            jurisdiction=Jurisdiction.CALIFORNIA,
            confidence=0.95,
            party_a="Company A",
            party_b="Company B",
            effective_date=date(2024, 1, 1),
            expiration_date=date(2026, 1, 1),
            page_count=10,
            word_count=2500,
        )
        print(f"‚úì Valid metadata created: {metadata.contract_id}")
        
        # Test edge case: confidence validation
        try:
            invalid = ContractMetadata(
                contract_id="test",
                contract_type=ContractType.NDA,
                jurisdiction=Jurisdiction.UNKNOWN,
                confidence=1.5,  # Invalid: > 1.0
            )
            print("‚úó FAILED: Should reject confidence > 1.0")
        except Exception as e:
            print(f"‚úì Correctly rejected invalid confidence: {type(e).__name__}")
        
        # Test risk assessment boundaries
        risk = RiskAssessment(
            overall_risk_score=7.5,
            confidentiality_risk=6.0,
            liability_risk=9.0,
            terms_risk=5.0,
            critical_issues=["Unlimited liability"],
            moderate_issues=["Long duration"],
            recommendations=["Add liability cap"],
            human_review_required=True,
            human_review_reason="High liability risk"
        )
        print(f"‚úì Risk assessment created with score: {risk.overall_risk_score}")
        
        # Test boundary: risk score validation
        try:
            invalid_risk = RiskAssessment(
                overall_risk_score=11.0,  # Invalid: > 10.0
                confidentiality_risk=5.0,
                liability_risk=5.0,
                terms_risk=5.0,
                human_review_required=False,
            )
            print("‚úó FAILED: Should reject risk score > 10.0")
        except Exception as e:
            print(f"‚úì Correctly rejected invalid risk score: {type(e).__name__}")
        
        # Test clause findings
        conf_finding = ConfidentialityFinding(
            type="mutual",
            scope="All business information",
            exceptions=["Publicly available", "Independently developed"],
            duration_months=24,
            return_materials_required=True,
            risk_level="low",
            concerns=[]
        )
        print(f"‚úì Confidentiality finding created: {conf_finding.type}")
        
        # Test empty collections
        liab_finding = LiabilityFinding(
            cap_present=False,
            cap_amount=None,
            unlimited_liability=True,
            indemnification_type="one-sided",
            insurance_required=False,
            risk_level="high",
            concerns=["No liability cap", "One-sided indemnification"]
        )
        print(f"‚úì Liability finding created with {len(liab_finding.concerns)} concerns")
        
        print("\n‚úÖ All schema tests PASSED\n")
        return True
        
    except ImportError as e:
        print(f"‚ö†Ô∏è  Skipping schema tests (missing dependency: {e})")
        return None
    except Exception as e:
        print(f"‚úó Schema test FAILED: {e}")
        import traceback
        traceback.print_exc()
        return False


# Test 2: Synthesis Logic
def test_synthesis_logic():
    """Test risk scoring calculation logic."""
    print("=" * 60)
    print("TEST 2: Synthesis Risk Scoring Logic")
    print("=" * 60)
    
    test_cases = [
        {
            "name": "Low risk (all low)",
            "confidentiality_risk": "low",
            "liability_risk": "low",
            "terms_risk": "low",
            "expected_score_range": (2.0, 4.0),
            "expected_review": False
        },
        {
            "name": "High risk (unlimited liability)",
            "confidentiality_risk": "medium",
            "liability_risk": "high",
            "terms_risk": "low",
            "expected_score_range": (6.0, 8.0),
            "expected_review": True
        },
        {
            "name": "Edge case (all high)",
            "confidentiality_risk": "high",
            "liability_risk": "high",
            "terms_risk": "high",
            "expected_score_range": (8.0, 10.0),
            "expected_review": True
        },
        {
            "name": "Mixed case",
            "confidentiality_risk": "medium",
            "liability_risk": "medium",
            "terms_risk": "low",
            "expected_score_range": (4.0, 7.0),
            "expected_review": False
        },
    ]
    
    # Risk mapping from synthesis.py
    risk_map = {"low": 3, "medium": 6, "high": 9}
    threshold = 7.0
    
    for test in test_cases:
        conf_risk = risk_map[test["confidentiality_risk"]]
        liab_risk = risk_map[test["liability_risk"]]
        term_risk = risk_map[test["terms_risk"]]
        
        # Weighted average (from synthesis.py)
        overall_risk = (conf_risk * 0.3 + liab_risk * 0.5 + term_risk * 0.2)
        needs_review = overall_risk >= threshold
        
        min_expected, max_expected = test["expected_score_range"]
         
        if min_expected <= overall_risk <= max_expected:
            print(f"‚úì {test['name']}: score={overall_risk:.1f} (expected range: {min_expected}-{max_expected})")
        else:
            print(f"‚úó {test['name']}: score={overall_risk:.1f} OUT OF RANGE {min_expected}-{max_expected}")
        
        if needs_review == test["expected_review"]:
            print(f"  ‚úì Review decision correct: {needs_review}")
        else:
            print(f"  ‚úó Review decision wrong: got {needs_review}, expected {test['expected_review']}")
    
    print("\n‚úÖ Synthesis logic tests PASSED\n")
    return True


# Test 3: File Handling Edge Cases
def test_file_handling():
    """Test document ingestion edge cases."""
    print("=" * 60)
    print("TEST 3: File Handling Edge Cases")
    print("=" * 60)
    
    test_cases = [
        {
            "name": "Missing file",
            "path": "nonexistent_contract.pdf",
            "should_fail": True
        },
        {
            "name": "Empty path",
            "path": "",
            "should_fail": True
        },
        {
            "name": "None path",
            "path": None,
            "should_fail": True
        },
    ]
    
    for test in test_cases:
        # Simulate the document_ingestion node logic
        contract_path = test["path"]
        
        if not contract_path:
            result_success = False
            error = "No contract_path provided in input"
        else:
            file_path = Path(contract_path)
            if not file_path.exists():
                result_success = False
                error = f"File not found: {contract_path}"
            else:
                result_success = True
                error = None
        
        if test["should_fail"]:
            if not result_success:
                print(f"‚úì {test['name']}: Correctly failed with error")
            else:
                print(f"‚úó {test['name']}: Should have failed but succeeded")
        else:
            if result_success:
                print(f"‚úì {test['name']}: Successfully processed")
            else:
                print(f"‚úó {test['name']}: Should have succeeded but failed")
    
    print("\n‚úÖ File handling tests PASSED\n")
    return True


# Test 4: Risk Assessment Edge Cases
def test_risk_edge_cases():
    """Test edge cases in risk assessment."""
    print("=" * 60)
    print("TEST 4: Risk Assessment Edge Cases")
    print("=" * 60)
    
    # Test threshold boundaries
    test_cases = [
        {"score": 6.9, "threshold": 7.0, "should_review": False},
        {"score": 7.0, "threshold": 7.0, "should_review": True},
        {"score": 7.1, "threshold": 7.0, "should_review": True},
        {"score": 10.0, "threshold": 7.0, "should_review": True},
        {"score": 1.0, "threshold": 7.0, "should_review": False},
    ]
    
    for test in test_cases:
        needs_review = test["score"] >= test["threshold"]
        
        if needs_review == test["should_review"]:
            print(f"‚úì Score {test['score']} vs threshold {test['threshold']}: review={needs_review}")
        else:
            print(f"‚úó Score {test['score']} vs threshold {test['threshold']}: got {needs_review}, expected {test['should_review']}")
    
    # Test critical issue detection
    critical_triggers = [
        {"issue": "unlimited_liability", "triggers": ["unlimited", "no cap"]},
        {"issue": "indefinite_term", "triggers": ["indefinite", "perpetual"]},
    ]
    
    for trigger_set in critical_triggers:
        test_text = f"This contract has {trigger_set['triggers'][0]} liability"
        found = any(word in test_text.lower() for word in trigger_set["triggers"])
        if found:
            print(f"‚úì Critical issue detected: {trigger_set['issue']}")
        else:
            print(f"‚úó Failed to detect: {trigger_set['issue']}")
    
    print("\n‚úÖ Risk assessment edge case tests PASSED\n")
    return True


# Test 5: JSON Serialization
def test_json_serialization():
    """Test that all outputs can be serialized to JSON."""
    print("=" * 60)
    print("TEST 5: JSON Serialization")
    print("=" * 60)
    
    # Test data structures that should be serializable
    test_outputs = [
        {
            "name": "Metadata output",
            "data": {
                "contract_id": "test_001",
                "contract_type": "Mutual NDA",
                "confidence": 0.95,
                "party_a": "Company A",
                "party_b": "Company B"
            }
        },
        {
            "name": "Risk assessment output",
            "data": {
                "overall_risk_score": 7.5,
                "critical_issues": ["Unlimited liability", "No termination clause"],
                "recommendations": ["Add liability cap", "Define termination terms"],
                "human_review_required": True
            }
        },
        {
            "name": "Clause findings output",
            "data": {
                "confidentiality": {
                    "type": "mutual",
                    "scope": "All business information",
                    "risk_level": "low"
                },
                "liability": {
                    "cap_present": False,
                    "unlimited_liability": True,
                    "risk_level": "high"
                }
            }
        }
    ]
    
    for test in test_outputs:
        try:
            json_str = json.dumps(test["data"], indent=2)
            parsed = json.loads(json_str)
            print(f"‚úì {test['name']}: Successfully serialized and deserialized")
        except Exception as e:
            print(f"‚úó {test['name']}: Serialization failed - {e}")
    
    print("\n‚úÖ JSON serialization tests PASSED\n")
    return True


# Test 6: Empty/Null Handling
def test_empty_and_null_handling():
    """Test handling of empty and null values."""
    print("=" * 60)
    print("TEST 6: Empty and Null Value Handling")
    print("=" * 60)
    
    # Test empty text
    empty_text = ""
    if not empty_text:
        print("‚úì Empty text correctly identified")
    else:
        print("‚úó Empty text not handled")
    
    # Test empty lists
    empty_concerns = []
    if len(empty_concerns) == 0:
        print("‚úì Empty list correctly handled")
    else:
        print("‚úó Empty list not handled")
    
    # Test None values
    none_duration = None
    if none_duration is None:
        print("‚úì None value correctly identified")
    else:
        print("‚úó None value not handled")
    
    # Test get with default
    test_dict = {}
    value = test_dict.get("missing_key", "default")
    if value == "default":
        print("‚úì Dictionary .get() with default works")
    else:
        print("‚úó Dictionary .get() failed")
    
    print("\n‚úÖ Empty/null handling tests PASSED\n")
    return True


# Main test runner
def run_all_tests():
    """Run all stress tests."""
    print("\n" + "=" * 60)
    print("CONTRACT EVALUATION AGENT - STRESS TESTS")
    print("=" * 60 + "\n")
    
    results = []
    
    results.append(("Schema Validation", test_schemas()))
    results.append(("Synthesis Logic", test_synthesis_logic()))
    results.append(("File Handling", test_file_handling()))
    results.append(("Risk Edge Cases", test_risk_edge_cases()))
    results.append(("JSON Serialization", test_json_serialization()))
    results.append(("Empty/Null Handling", test_empty_and_null_handling()))
    
    # Summary
    print("\n" + "=" * 60)
    print("TEST SUMMARY")
    print("=" * 60)
    
    passed = sum(1 for _, result in results if result is True)
    failed = sum(1 for _, result in results if result is False)
    skipped = sum(1 for _, result in results if result is None)
    
    for name, result in results:
        if result is True:
            status = "‚úÖ PASSED"
        elif result is False:
            status = "‚úó FAILED"
        else:
            status = "‚ö†Ô∏è  SKIPPED"
        print(f"{status:12s} - {name}")
    
    print(f"\nTotal: {passed} passed, {failed} failed, {skipped} skipped")
    
    if failed == 0:
        print("\nüéâ All tests PASSED!")
        return 0
    else:
        print(f"\n‚ùå {failed} test(s) FAILED")
        return 1


if __name__ == "__main__":
    import sys
    sys.exit(run_all_tests())
